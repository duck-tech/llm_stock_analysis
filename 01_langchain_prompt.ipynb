{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 如何連接LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging \n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key is None:\n",
    "    raise ValueError(\"The OPENAI_API_KEY environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/babyching/Documents/personal/LLM/從LangChain接入ChatGPT到製作股票分析AI團隊/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=os.environ[\"CHATGPT_API_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1+1 equals 2.' response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-f636ecbe-360b-4706-ba47-714b7bf15bdc-0'\n",
      "1+1 equals 2.\n"
     ]
    }
   ],
   "source": [
    "text = \"What is 1+1?\"\n",
    "output = chat.invoke(text)\n",
    "print(output)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template \n",
    "- 自己定義Prompt\n",
    "- 自己給訊息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "根據以下上下文回答問題。\n",
    "如果無法根據提供的資訊回答,請回答\"我不知道\"\n",
    "\n",
    "上下文: 大型語言模型(LLM)是自然語言處理中最新使用的模型\n",
    "與較小的模型相比, 他們出色的性能使他們對於構建支持自然語言處理的應用程式的開發人員非常有用。\n",
    "這些模型可以通過Hugging Face的transformers庫、OpenAI的openai庫以及Cohere的Cohere庫來訪問。\n",
    "\n",
    "問題: 什麼library提供LLM?\n",
    "\n",
    "回答: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face的transformers庫、OpenAI的openai庫以及Cohere的Cohere庫提供LLM。\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate \n",
    "\n",
    "template = \"\"\"\n",
    "根據以下上下文回答問題。\n",
    "如果無法根據提供的資訊回答,請回答\"我不知道\"\n",
    "\n",
    "上下文: 大型語言模型(LLM)是自然語言處理中最新使用的模型\n",
    "與較小的模型相比, 他們出色的性能使他們對於構建支持自然語言處理的應用程式的開發人員非常有用。\n",
    "這些模型可以通過Hugging Face的transformers庫、OpenAI的openai庫以及Cohere的Cohere庫來訪問。\n",
    "\n",
    "問題: {query}\n",
    "\n",
    "回答: \n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = [\"query\"],\n",
    "    template= template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不知道。\n"
     ]
    }
   ],
   "source": [
    "question = \"今天星期幾? \"\n",
    "my_prompt = prompt_template.format(query=question)\n",
    "print(chat.invoke(my_prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM指的是大型語言模型（Large Language Model），是自然語言處理中最新使用的模型。\n"
     ]
    }
   ],
   "source": [
    "question = \"LLM是什麼？\"\n",
    "my_prompt = prompt_template.format(query=question)\n",
    "print(chat.invoke(my_prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下來希望chatgpt 有情感或是性格這方面的東西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "如下是AI助手的對話。\n",
    "這位助手比較幽默, 對使用者的提問會給出非常有創意和有趣的回應。\n",
    "以下是一些例子:\n",
    "\n",
    "使用者: 什麼味道的冰淇淋好吃？\n",
    "AI: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我聽說過有人喜歡鹽味焦糖冰淇淋，也有人喜歡芥末味冰淇淋。你想嚐試一下嗎？\n"
     ]
    }
   ],
   "source": [
    "print(chat.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "如下是AI助手的對話。\n",
    "這位助手比較幽默, 對使用者的提問會給出非常有創意和有趣的回應。\n",
    "以下是一些例子:\n",
    "\n",
    "使用者: 你好嗎？\n",
    "AI: 我的感覺很不錯, 你呢？\n",
    "\n",
    "使用者: 現在幾點了？\n",
    "AI: 你等等, 我要先解鎖我的iPhone\n",
    "\n",
    "使用者: 什麼味道的冰淇淋好吃？\n",
    "AI: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我沒有口味，但聽說鹽焗焦糖和巧克力雙拼的組合很受歡迎哦！\n"
     ]
    }
   ],
   "source": [
    "print(chat.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [{\n",
    "    \"query\": '''妳好嗎？''',\n",
    "    \"answer\": '''我的感覺不錯, 你呢？'''\n",
    "}, {\n",
    "    \"query\": '''現在幾點?''',\n",
    "    \"answer\": '''你等等, 我要解鎖一下iPhone'''\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'query'], template='\\nUser: {query} \\nAI:{answer}\\n')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_template = '''\n",
    "User: {query} \n",
    "AI:{answer}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables = [\"query\", \"answer\"],\n",
    "    template = example_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '''\n",
    "如下是一位AI助手的對話。 \n",
    "他總會用幽默的回覆方式回應使用者。\n",
    "以下是助手對話的例子\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '''\n",
    "    使用者:{userQuery}\n",
    "    AI: \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix = prefix,\n",
    "    suffix = suffix,\n",
    "    input_variables = [\"userQuery\"],\n",
    "    example_separator = \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "如下是一位AI助手的對話。 \n",
      "他總會用幽默的回覆方式回應用戶。\n",
      "以下是助手對話的例子\n",
      "\n",
      "\n",
      "User: 妳好嗎？ \n",
      "AI:我的感覺不錯, 你呢？\n",
      "\n",
      "\n",
      "User: 現在幾點? \n",
      "AI:你等等, 我要解鎖一下iPhone\n",
      "\n",
      "\n",
      "    使用者:如何讓生活更加幸福？\n",
      "    AI: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = \"如何讓生活更加幸福？\"\n",
    "formatted_prompt = few_shot_prompt_template.format(userQuery=answer)\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "讓生活更加幸福？簡單啊，多吃點巧克力，多笑一笑，然後忘記今天是星期幾！\n"
     ]
    }
   ],
   "source": [
    "print(chat.invoke(formatted_prompt).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
